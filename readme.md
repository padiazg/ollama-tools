# Ollama tools
A few tools to iteract with your local Ollama server.

This project started as a couriosity to know how much memory do I need to run the models I had downloaded as some where almost unusable no matter the specs from my hardware.

I must mention that I'm not an expert in this matter, and most of the logics and formulas were inspired by the code from this repo [ollama-gpu-calculator](https://github.com/aleibovici/ollama-gpu-calculator)

## The theory
Check the theory [here](theory.md)

## Install

